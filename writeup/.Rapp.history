x = rt.data
rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100 & Accuracy != 0))
nrow(rt.data)
rt.data$Accuracy
rt.data = x
which(is.na(rt.data$Accuracy))
rt.data$Accuracy[which(is.na(rt.data$Accuracy))] = 1
rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100 & Accuracy == 1)
)
nrow(rt.data)
regions = data.frame(Region=c(1:7),ROI=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data = merge(rt.data,regions,by="Region")rt.data = zscoreRegion(rt.data,cutoff=3)rt.data$ROI = factor(rt.data$ROI,levels=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data$logRT = log(rt.data$RT)######## Get by participant means, SEs for raw RTs. SEs are computed using Bakeman + McArthur correction.data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=RT)data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
nrow(rt.data)
rm(list=ls())setwd("~/Dropbox/CSL/Agreement and Reflexives/Online_Replication/")library(ez)library(ggplot2)library(plyr)library(ggthemes)library(lme4)library(MASS)library(ggmap)source("SPRfnx.R")######## Read in and parse results file; column names may need to be adjusted if this script is being adapted to new experimentsmycols = c("Subject","MD5","TrialType","Number","Element","Experiment","Item","Region","Response","RT","Judgment")results=read.csv('results_6Ss.txt',header = 0, sep = ",", comment.char = "#",col.names=mycols);summary(results)################################################################################  ######## This part parses the biographical data. It attempts to identify:######## 		i) non-native speakers######## 		ii) participants who have taken the test more than once######## 		iii) participants who got the instruction questions incorrect ######## ######## 		it makes a list 'good.subjects' that contains participants who will be included in the analysis.######## ################################################################################  ######## First get biographical databiography = droplevels(subset(results,Experiment=="intro"|Experiment=="exit"))biography = biography[,c("Subject","Region","Response")]colnames(biography) = c("Subject","Question","Response")biography = reshape(biography,direction="wide",idvar="Subject",v.names="Response",timevar="Question")summary(biography)######## Rename columns and pull out subset of relevant fieldscolnames(biography) = c("Subject","Age","NativeLang","State","ParentLang","DominantLang","OtherLang","Consent","Gender","Question1","Question2","Question3","Question4","Question5","Question6","RandomField","Worker_ID","Identifier")short.biography = biography[,c("Subject","NativeLang","ParentLang","Age","Gender","State","Question1","Question2","Question3","Question4","Question5","Question6","Worker_ID")]short.biography$Age = as.integer(as.character(short.biography$Age))short.biography$SID = c(1:nrow(short.biography))######## Check that the length of unique worker IDs matches length of # of subjects######## 6 participants removed on this criterion.nrow(short.biography) == length(unique(short.biography$Worker_ID))good.subjects = short.biography$SID[!(duplicated(short.biography$Worker_ID))]######## Check for accuracy on instruction questions. Remove participants who answered any incorrectly.######## 11 participants removed on this criterioncorrect.answers = c("space","silent","no","j","yes","maximized")names(correct.answers) = c("Question1","Question2","Question3","Question4","Question5","Question6")all.answers = as.matrix(short.biography[,c("Question1","Question2","Question3","Question4","Question5","Question6")])accurate.answers = apply(all.answers, 1, identical, correct.answers) good.subjects = intersect(good.subjects,which(accurate.answers))######## Leaves us with an N of 126.######## Age analysis: there is a mode of participants 50+. Range = 19-68.dev.new(width =10.761468,height = 4.477064)hg_dot(short.biography$Age[short.biography$SID %in% good.subjects],breaks = seq(17.5,77.5,by=1),cex=2)######## Location analysis######## Someone put in 'ON'. Assumed it was typo for 'OH'states = tolower(short.biography$State)states =  gsub(" $","", states, perl=T)for (i in 1:length(states)) {	if (nchar(states[i]) > 2) {		states[i] = state.abb[match(states[i] ,tolower(state.name))]	}}states = toupper(states)states = gsub("ON","OH",states)state.numbers = table(states[good.subjects])state.data = data.frame(states = state.numbers)state.data$x = state.center$x[match(state.data$states.Var1 ,state.abb)]state.data$y = state.center$y[match(state.data$states.Var1 ,state.abb)]names(state.data) = c("State","Frequency","x","y")dev.new(width =10.752293,height = 10.752293)map = get_map(location = 'USA', zoom = 4,maptype="roadmap")ggmap(map) + geom_point(aes(x = x, y = y, size = Frequency), data = state.data, alpha = .5)+ scale_size(range = c(0, 30))################################################################################  ######## This part turns to analysis of experimental data:######## ################################################################################  ######## Test latin squaring; check... only a tad off.items = droplevels(subset(results,TrialType == "QuestionAlt"))xtabs(~Item+Experiment,data=items)######## Merge biography information into data; create factors, trialIDs. Remove all bad subjects identified above.results = droplevels(subset(results,SID in good.subjects))results = merge(results,short.biography,by.x="Subject",by.y="Subject")results$Length = nchar(as.character(results$Response))results$Subject = as.factor(results$Subject)results$Item = as.factor(results$Item)results$RT = as.numeric(as.character(results$RT))results$TrialId = paste(results$SID,results$Item,sep=":")results$cLength = results$Length - mean(results$Length)######## Separate data into accuracy data and rt.dataaccuracy.data = droplevels(subset(results,TrialType == "QuestionAlt"))rt.data = droplevels(subset(results, TrialType == "DashedSentence"))rt.data$NumRegion = as.numeric(as.character(rt.data$Region))rt.data$logRegion = log(rt.data$NumRegion)rt.data$cLogRegion = rt.data$logRegion - mean(rt.data$logRegion)################################################################################  ######## Analysis of accuracy data:######## ################################################################################  condition.key = data.frame(	Experiment = c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8")	,Dep=c("Agr","Agr","Agr","Agr","Ref","Ref","Ref","Ref"),	Gram=c("G","G","U","U","G","G","U","U"),	Intr=c("S","P","S","P","S","P","S","P"))accuracy.data = droplevels(merge(accuracy.data,condition.key,by.x = "Experiment", by.y = "Experiment", sort=F))######## Remove questions, test accuracycontrasts(accuracy.data$Dep) = contr.sum(levels(accuracy.data$Dep))/2contrasts(accuracy.data$Gram) = contr.sum(levels(accuracy.data$Gram))/2contrasts(accuracy.data$Intr) = contr.sum(levels(accuracy.data$Intr))/2######## Mean response accuracy, tabulated by conditiontapply(accuracy.data$RT,list(accuracy.data$Gram,accuracy.data$Intr,accuracy.data$Dep),mean)######## By participant accuracy, to remove under-performing Sssubj.accuracies = sort(tapply(accuracy.data$RT,list(accuracy.data$Subject),mean))bad.subjects = names(subj.accuracies[which(subj.accuracies < .70)])######## Analysis of accuracy data.#acc.model = glmer(RT~Dep*Gram*Intr+(1|Subject)+(1|Item),data=accuracy.data,family="binomial")######## Tag trials with accuracy######## TO DO: Check that this is working propery. accuracy.key = data.frame(TrialId = unique(results$TrialId),Accuracy = 'NA')accuracies = accuracy.data$RTnames(accuracies) = accuracy.data$TrialIdaccuracy.key$Accuracy = accuracies[accuracy.key$TrialId]rt.data = merge(rt.data,accuracy.key,by=c("TrialId"))################################################################################  ######## Analysis of RT data:######## ################################################################################  ######## What is RT distribution?truehist(rt.data$RT)######## Identify Ss with reading strategies that indicate lack of comprehension.subjrt.sds = tapply(rt.data$RT,list(rt.data$Subject),sd)bad.subjects = c(bad.subjects,names(subjrt.sds[which(subjrt.sds < 50)]))######## Remove all participants identified in previous stages of data cleaning. 119 Participants leftrt.data = droplevels(subset(rt.data,SID %in% good.subjects & !(Subject %in% bad.subjects)))######## Fit residualization model to filler data######## Data exploration suggests log of region, rather than straight region, would be better fit. There is large discrepency between average RTs on first region and those on subsequent regions######## Also leaving in interaction term of length and region. Model suggests that effect of length is significantly modulated by position in sentence, with less of an impact later on.######## TO DO: Implement LMER. Prediction isn't working yet, needs to be updated. resid.Model = lmer(RT~cLength*cLogRegion+(1|Subject),data=subset(rt.data, Experiment == 'f'))######## With residual model fit, now get rid of all filler / practice trialsrt.data = droplevels(subset(rt.data, Experiment %in% c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8")))rt.data$residRT = rt.data$RT - predict(resid.Model,rt.data)######## Put in contrasts; simple difference coding.rt.data = droplevels(merge(rt.data,condition.key,by.x = "Experiment", by.y = "Experiment", sort=F))######## Outlier rejection. Trim RTs above 5000 and below 100, and apply z-score of 3 by region.rt.data$Accuracy[which(is.na(rt.data$Accuracy))] = 1
str(rt.data)
which(rt.data$RT < 100)
bad.trials = unique(rt.data$TrialId[which(rt.data$RT < 100)])
bad.trials
rm(list=ls())setwd("~/Dropbox/CSL/Agreement and Reflexives/Online_Replication/")library(ez)library(ggplot2)library(plyr)library(ggthemes)library(lme4)library(MASS)library(ggmap)source("SPRfnx.R")######## Read in and parse results file; column names may need to be adjusted if this script is being adapted to new experimentsmycols = c("Subject","MD5","TrialType","Number","Element","Experiment","Item","Region","Response","RT","Judgment")results=read.csv('results_6Ss.txt',header = 0, sep = ",", comment.char = "#",col.names=mycols);summary(results)################################################################################  ######## This part parses the biographical data. It attempts to identify:######## 		i) non-native speakers######## 		ii) participants who have taken the test more than once######## 		iii) participants who got the instruction questions incorrect ######## ######## 		it makes a list 'good.subjects' that contains participants who will be included in the analysis.######## ################################################################################  ######## First get biographical databiography = droplevels(subset(results,Experiment=="intro"|Experiment=="exit"))biography = biography[,c("Subject","Region","Response")]colnames(biography) = c("Subject","Question","Response")biography = reshape(biography,direction="wide",idvar="Subject",v.names="Response",timevar="Question")summary(biography)######## Rename columns and pull out subset of relevant fieldscolnames(biography) = c("Subject","Age","NativeLang","State","ParentLang","DominantLang","OtherLang","Consent","Gender","Question1","Question2","Question3","Question4","Question5","Question6","RandomField","Worker_ID","Identifier")short.biography = biography[,c("Subject","NativeLang","ParentLang","Age","Gender","State","Question1","Question2","Question3","Question4","Question5","Question6","Worker_ID")]short.biography$Age = as.integer(as.character(short.biography$Age))short.biography$SID = c(1:nrow(short.biography))######## Check that the length of unique worker IDs matches length of # of subjects######## 6 participants removed on this criterion.nrow(short.biography) == length(unique(short.biography$Worker_ID))good.subjects = short.biography$SID[!(duplicated(short.biography$Worker_ID))]######## Check for accuracy on instruction questions. Remove participants who answered any incorrectly.######## 11 participants removed on this criterioncorrect.answers = c("space","silent","no","j","yes","maximized")names(correct.answers) = c("Question1","Question2","Question3","Question4","Question5","Question6")all.answers = as.matrix(short.biography[,c("Question1","Question2","Question3","Question4","Question5","Question6")])accurate.answers = apply(all.answers, 1, identical, correct.answers) good.subjects = intersect(good.subjects,which(accurate.answers))######## Leaves us with an N of 126.######## Age analysis: there is a mode of participants 50+. Range = 19-68.dev.new(width =10.761468,height = 4.477064)hg_dot(short.biography$Age[short.biography$SID %in% good.subjects],breaks = seq(17.5,77.5,by=1),cex=2)######## Location analysis######## Someone put in 'ON'. Assumed it was typo for 'OH'states = tolower(short.biography$State)states =  gsub(" $","", states, perl=T)for (i in 1:length(states)) {	if (nchar(states[i]) > 2) {		states[i] = state.abb[match(states[i] ,tolower(state.name))]	}}states = toupper(states)states = gsub("ON","OH",states)state.numbers = table(states[good.subjects])state.data = data.frame(states = state.numbers)state.data$x = state.center$x[match(state.data$states.Var1 ,state.abb)]state.data$y = state.center$y[match(state.data$states.Var1 ,state.abb)]names(state.data) = c("State","Frequency","x","y")dev.new(width =10.752293,height = 10.752293)map = get_map(location = 'USA', zoom = 4,maptype="roadmap")ggmap(map) + geom_point(aes(x = x, y = y, size = Frequency), data = state.data, alpha = .5)+ scale_size(range = c(0, 30))################################################################################  ######## This part turns to analysis of experimental data:######## ################################################################################  ######## Test latin squaring; check... only a tad off.items = droplevels(subset(results,TrialType == "QuestionAlt"))xtabs(~Item+Experiment,data=items)######## Merge biography information into data; create factors, trialIDs. Remove all bad subjects identified above.results = droplevels(subset(results,SID in good.subjects))results = merge(results,short.biography,by.x="Subject",by.y="Subject")results$Length = nchar(as.character(results$Response))results$Subject = as.factor(results$Subject)results$Item = as.factor(results$Item)results$RT = as.numeric(as.character(results$RT))results$TrialId = paste(results$SID,results$Item,sep=":")results$cLength = results$Length - mean(results$Length)######## Separate data into accuracy data and rt.dataaccuracy.data = droplevels(subset(results,TrialType == "QuestionAlt"))rt.data = droplevels(subset(results, TrialType == "DashedSentence"))rt.data$NumRegion = as.numeric(as.character(rt.data$Region))rt.data$logRegion = log(rt.data$NumRegion)rt.data$cLogRegion = rt.data$logRegion - mean(rt.data$logRegion)################################################################################  ######## Analysis of accuracy data:######## ################################################################################  condition.key = data.frame(	Experiment = c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8")	,Dep=c("Agr","Agr","Agr","Agr","Ref","Ref","Ref","Ref"),	Gram=c("G","G","U","U","G","G","U","U"),	Intr=c("S","P","S","P","S","P","S","P"))accuracy.data = droplevels(merge(accuracy.data,condition.key,by.x = "Experiment", by.y = "Experiment", sort=F))######## Remove questions, test accuracycontrasts(accuracy.data$Dep) = contr.sum(levels(accuracy.data$Dep))/2contrasts(accuracy.data$Gram) = contr.sum(levels(accuracy.data$Gram))/2contrasts(accuracy.data$Intr) = contr.sum(levels(accuracy.data$Intr))/2######## Mean response accuracy, tabulated by conditiontapply(accuracy.data$RT,list(accuracy.data$Gram,accuracy.data$Intr,accuracy.data$Dep),mean)######## By participant accuracy, to remove under-performing Sssubj.accuracies = sort(tapply(accuracy.data$RT,list(accuracy.data$Subject),mean))bad.subjects = names(subj.accuracies[which(subj.accuracies < .70)])######## Analysis of accuracy data.#acc.model = glmer(RT~Dep*Gram*Intr+(1|Subject)+(1|Item),data=accuracy.data,family="binomial")######## Tag trials with accuracy######## TO DO: Check that this is working propery. accuracy.key = data.frame(TrialId = unique(results$TrialId),Accuracy = 'NA')accuracies = accuracy.data$RTnames(accuracies) = accuracy.data$TrialIdaccuracy.key$Accuracy = accuracies[accuracy.key$TrialId]rt.data = merge(rt.data,accuracy.key,by=c("TrialId"))################################################################################  ######## Analysis of RT data:######## ################################################################################  ######## What is RT distribution?truehist(rt.data$RT)######## Identify Ss with reading strategies that indicate lack of comprehension.subjrt.sds = tapply(rt.data$RT,list(rt.data$Subject),sd)bad.subjects = c(bad.subjects,names(subjrt.sds[which(subjrt.sds < 50)]))######## Remove all participants identified in previous stages of data cleaning. 119 Participants leftrt.data = droplevels(subset(rt.data,SID %in% good.subjects & !(Subject %in% bad.subjects)))######## Fit residualization model to filler data######## Data exploration suggests log of region, rather than straight region, would be better fit. There is large discrepency between average RTs on first region and those on subsequent regions######## Also leaving in interaction term of length and region. Model suggests that effect of length is significantly modulated by position in sentence, with less of an impact later on.######## TO DO: Implement LMER. Prediction isn't working yet, needs to be updated. resid.Model = lmer(RT~cLength*cLogRegion+(1|Subject),data=subset(rt.data, Experiment == 'f'))######## With residual model fit, now get rid of all filler / practice trialsrt.data = droplevels(subset(rt.data, Experiment %in% c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8")))rt.data$residRT = rt.data$RT - predict(resid.Model,rt.data)######## Put in contrasts; simple difference coding.rt.data = droplevels(merge(rt.data,condition.key,by.x = "Experiment", by.y = "Experiment", sort=F))######## Outlier rejection. Trim RTs above 5000 and below 100, and apply z-score of 3 by region.######## Also remove 'bad trials', trials where a single region was read for less than 100 ms
x = rt.data
rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100 & (TrialId %in% bad.trials))regions = data.frame(Region=c(1:7),ROI=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data = merge(rt.data,regions,by="Region")rt.data = zscoreRegion(rt.data,cutoff=3)rt.data$ROI = factor(rt.data$ROI,levels=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data$logRT = log(rt.data$RT)######## Get by participant means, SEs for raw RTs. SEs are computed using Bakeman + McArthur correction.data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=RT)data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
bad.trials = unique(rt.data$TrialId[which(rt.data$RT < 100)])rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100 & (TrialId %in% bad.trials))regions = data.frame(Region=c(1:7),ROI=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data = merge(rt.data,regions,by="Region")rt.data = zscoreRegion(rt.data,cutoff=3)rt.data$ROI = factor(rt.data$ROI,levels=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data$logRT = log(rt.data$RT)######## Get by participant means, SEs for raw RTs. SEs are computed using Bakeman + McArthur correction.data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=RT)data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100 & !(TrialId %in% bad.trials))regions = data.frame(Region=c(1:7),ROI=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data = merge(rt.data,regions,by="Region")rt.data = zscoreRegion(rt.data,cutoff=3)rt.data$ROI = factor(rt.data$ROI,levels=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data$logRT = log(rt.data$RT)######## Get by participant means, SEs for raw RTs. SEs are computed using Bakeman + McArthur correction.data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=RT)data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
bad.trials = unique(rt.data$TrialId[which(rt.data$RT < 100)])
rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100 & !(TrialId %in% bad.trials))
)
regions = data.frame(Region=c(1:7),ROI=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data = merge(rt.data,regions,by="Region")rt.data = zscoreRegion(rt.data,cutoff=3)rt.data$ROI = factor(rt.data$ROI,levels=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data$logRT = log(rt.data$RT)######## Get by participant means, SEs for raw RTs. SEs are computed using Bakeman + McArthur correction.data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=RT)data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
rt.data$Accuracy[which(is.na(rt.data$Accuracy))] = 1
rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100 & Accuracy == 1 & !(TrialId %in% bad.trials)))regions = data.frame(Region=c(1:7),ROI=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data = merge(rt.data,regions,by="Region")rt.data = zscoreRegion(rt.data,cutoff=3)rt.data$ROI = factor(rt.data$ROI,levels=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data$logRT = log(rt.data$RT)######## Get by participant means, SEs for raw RTs. SEs are computed using Bakeman + McArthur correction.data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=RT)data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
rt.data = x
rt.data$Accuracy[which(is.na(rt.data$Accuracy))] = 1bad.trials = unique(rt.data$TrialId[which(rt.data$RT < 100)])rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100 & Accuracy == 1 & !(TrialId %in% bad.trials)))regions = data.frame(Region=c(1:7),ROI=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data = merge(rt.data,regions,by="Region")rt.data = zscoreRegion(rt.data,cutoff=3)rt.data$ROI = factor(rt.data$ROI,levels=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data$logRT = log(rt.data$RT)######## Get by participant means, SEs for raw RTs. SEs are computed using Bakeman + McArthur correction.data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=RT)data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
rt.data = x
rt.data$Accuracy[which(is.na(rt.data$Accuracy))] = 1bad.trials = unique(rt.data$TrialId[which(rt.data$RT < 100)])rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100 & !(TrialId %in% bad.trials)))regions = data.frame(Region=c(1:7),ROI=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data = merge(rt.data,regions,by="Region")rt.data = zscoreRegion(rt.data,cutoff=3)rt.data$ROI = factor(rt.data$ROI,levels=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data$logRT = log(rt.data$RT)######## Get by participant means, SEs for raw RTs. SEs are computed using Bakeman + McArthur correction.data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=RT)data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
rt.data = x
rt.data$Accuracy[which(is.na(rt.data$Accuracy))] = 1bad.trials = unique(rt.data$TrialId[which(rt.data$RT < 100)])rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100))regions = data.frame(Region=c(1:7),ROI=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data = merge(rt.data,regions,by="Region")rt.data = zscoreRegion(rt.data,cutoff=3)rt.data$ROI = factor(rt.data$ROI,levels=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data$logRT = log(rt.data$RT)######## Get by participant means, SEs for raw RTs. SEs are computed using Bakeman + McArthur correction.data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=RT)data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
by.subj.means = with(subset(rt.data,ROI=="V+DEP"),tapply(logRT,list(Subject,Experiment),mean))
by.subj.diffs = by.subj.means[,3]-by.subj.means[,4]
by.subj.diffs
names(by.subj.diffs)
by.subj.means = with(subset(rt.data,ROI=="V+DEP"),tapply(logRT,list(Subject,Experiment,Age),mean))by.subj.diffs = by.subj.means[,3]-by.subj.means[,4]
by.subj.diffs = by.subj.means[,3,]-by.subj.means[,4,]
by.subj.diffs
by.subj.means = with(subset(rt.data,ROI=="V+DEP"),tapply(logRT,list(Subject,Experiment),mean))by.subj.diffs = by.subj.means[,3,]-by.subj.means[,4]
by.subj.diffs = by.subj.means[,3,]-by.subj.means[,4]
by.subj.means = with(subset(rt.data,ROI=="V+DEP"),tapply(logRT,list(Subject,Experiment),mean))
by.subj.diffs = by.subj.means[,3,]-by.subj.means[,4]
by.subj.diffs = by.subj.means[,3]-by.subj.means[,4]
by.subj.ages = with(subset(rt.data,ROI=="V+DEP"),tapply(logRT,list(Subject,Age),mean))
by.subj.ages
by.subj.ages = with(subset(rt.data,ROI=="V+DEP"),tapply(Age,list(Subject),mean))
by.subj.ages
by
by.subj.diffs = cbind(by.subj.diffs,by.subj.ages)
by.subj.diffs
plot(by.subj.diffs[,2],by.subj.diffs[,1])
by.subj.diffs = by.subj.means[,7]-by.subj.means[,8]
by.subj.ages = with(subset(rt.data,ROI=="V+DEP"),tapply(Age,list(Subject),mean))by.subj.diffs = cbind(by.subj.diffs,by.subj.ages)
plot(by.subj.diffs[,2],by.subj.diffs[,1])
lm(by.subj.diffs[,1] ~by.subj.diffs[,2])
lm(by.subj.diffs[,1] ~by.subj.diffs[,2])->foo
summary(foo)
by.subj.diffs = by.subj.means[,3]-by.subj.means[,4]
plot(by.subj.diffs[,2],by.subj.diffs[,1])
by.subj.diffs = cbind(by.subj.diffs,by.subj.ages)
plot(by.subj.diffs[,2],by.subj.diffs[,1])
by.subj.diffs = by.subj.means[,7]-by.subj.means[,5]cohens.d = mean(by.subj.diffs)/sd(by.subj.diffs)by.subj.ages = with(subset(rt.data,ROI=="V+DEP"),tapply(Age,list(Subject),mean))by.subj.diffs = cbind(by.subj.diffs,by.subj.ages)
plot(by.subj.diffs[,2],by.subj.diffs[,1])
rm(list=ls())setwd("~/Dropbox/CSL/Agreement and Reflexives/Online_Replication/")library(ez)library(ggplot2)library(plyr)library(ggthemes)library(lme4)library(MASS)library(ggmap)source("SPRfnx.R")######## Read in and parse results file; column names may need to be adjusted if this script is being adapted to new experimentsmycols = c("Subject","MD5","TrialType","Number","Element","Experiment","Item","Region","Response","RT","Judgment")results=read.csv('results_6Ss.txt',header = 0, sep = ",", comment.char = "#",col.names=mycols);summary(results)################################################################################  ######## This part parses the biographical data. It attempts to identify:######## 		i) non-native speakers######## 		ii) participants who have taken the test more than once######## 		iii) participants who got the instruction questions incorrect ######## ######## 		it makes a list 'good.subjects' that contains participants who will be included in the analysis.######## ################################################################################  ######## First get biographical databiography = droplevels(subset(results,Experiment=="intro"|Experiment=="exit"))biography = biography[,c("Subject","Region","Response")]colnames(biography) = c("Subject","Question","Response")biography = reshape(biography,direction="wide",idvar="Subject",v.names="Response",timevar="Question")summary(biography)######## Rename columns and pull out subset of relevant fieldscolnames(biography) = c("Subject","Age","NativeLang","State","ParentLang","DominantLang","OtherLang","Consent","Gender","Question1","Question2","Question3","Question4","Question5","Question6","RandomField","Worker_ID","Identifier")short.biography = biography[,c("Subject","NativeLang","ParentLang","Age","Gender","State","Question1","Question2","Question3","Question4","Question5","Question6","Worker_ID")]short.biography$Age = as.integer(as.character(short.biography$Age))short.biography$SID = c(1:nrow(short.biography))######## Check that the length of unique worker IDs matches length of # of subjects######## 6 participants removed on this criterion.nrow(short.biography) == length(unique(short.biography$Worker_ID))good.subjects = short.biography$SID[!(duplicated(short.biography$Worker_ID))]######## Check for accuracy on instruction questions. Remove participants who answered any incorrectly.######## 11 participants removed on this criterioncorrect.answers = c("space","silent","no","j","yes","maximized")names(correct.answers) = c("Question1","Question2","Question3","Question4","Question5","Question6")all.answers = as.matrix(short.biography[,c("Question1","Question2","Question3","Question4","Question5","Question6")])accurate.answers = apply(all.answers, 1, identical, correct.answers) good.subjects = intersect(good.subjects,which(accurate.answers))######## Leaves us with an N of 126.######## Age analysis: there is a mode of participants 50+. Range = 19-68.dev.new(width =10.761468,height = 4.477064)hg_dot(short.biography$Age[short.biography$SID %in% good.subjects],breaks = seq(17.5,77.5,by=1),cex=2)######## Location analysis######## Someone put in 'ON'. Assumed it was typo for 'OH'states = tolower(short.biography$State)states =  gsub(" $","", states, perl=T)for (i in 1:length(states)) {	if (nchar(states[i]) > 2) {		states[i] = state.abb[match(states[i] ,tolower(state.name))]	}}states = toupper(states)states = gsub("ON","OH",states)state.numbers = table(states[good.subjects])state.data = data.frame(states = state.numbers)state.data$x = state.center$x[match(state.data$states.Var1 ,state.abb)]state.data$y = state.center$y[match(state.data$states.Var1 ,state.abb)]names(state.data) = c("State","Frequency","x","y")dev.new(width =10.752293,height = 10.752293)map = get_map(location = 'USA', zoom = 4,maptype="roadmap")ggmap(map) + geom_point(aes(x = x, y = y, size = Frequency), data = state.data, alpha = .5)+ scale_size(range = c(0, 30))################################################################################  ######## This part turns to analysis of experimental data:######## ################################################################################  ######## Test latin squaring; check... only a tad off.items = droplevels(subset(results,TrialType == "QuestionAlt"))xtabs(~Item+Experiment,data=items)######## Merge biography information into data; create factors, trialIDs. Remove all bad subjects identified above.results = droplevels(subset(results,SID in good.subjects))results = merge(results,short.biography,by.x="Subject",by.y="Subject")results$Length = nchar(as.character(results$Response))results$Subject = as.factor(results$Subject)results$Item = as.factor(results$Item)results$RT = as.numeric(as.character(results$RT))results$TrialId = paste(results$SID,results$Item,sep=":")results$cLength = results$Length - mean(results$Length)######## Separate data into accuracy data and rt.dataaccuracy.data = droplevels(subset(results,TrialType == "QuestionAlt"))rt.data = droplevels(subset(results, TrialType == "DashedSentence"))rt.data$NumRegion = as.numeric(as.character(rt.data$Region))rt.data$logRegion = log(rt.data$NumRegion)rt.data$cLogRegion = rt.data$logRegion - mean(rt.data$logRegion)################################################################################  ######## Analysis of accuracy data:######## ################################################################################  condition.key = data.frame(	Experiment = c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8")	,Dep=c("Agr","Agr","Agr","Agr","Ref","Ref","Ref","Ref"),	Gram=c("G","G","U","U","G","G","U","U"),	Intr=c("S","P","S","P","S","P","S","P"))accuracy.data = droplevels(merge(accuracy.data,condition.key,by.x = "Experiment", by.y = "Experiment", sort=F))######## Remove questions, test accuracycontrasts(accuracy.data$Dep) = contr.sum(levels(accuracy.data$Dep))/2contrasts(accuracy.data$Gram) = contr.sum(levels(accuracy.data$Gram))/2contrasts(accuracy.data$Intr) = contr.sum(levels(accuracy.data$Intr))/2######## Mean response accuracy, tabulated by conditiontapply(accuracy.data$RT,list(accuracy.data$Gram,accuracy.data$Intr,accuracy.data$Dep),mean)######## By participant accuracy, to remove under-performing Sssubj.accuracies = sort(tapply(accuracy.data$RT,list(accuracy.data$Subject),mean))bad.subjects = names(subj.accuracies[which(subj.accuracies < .70)])######## Analysis of accuracy data.#acc.model = glmer(RT~Dep*Gram*Intr+(1|Subject)+(1|Item),data=accuracy.data,family="binomial")######## Tag trials with accuracy######## TO DO: Check that this is working propery. accuracy.key = data.frame(TrialId = unique(results$TrialId),Accuracy = 'NA')accuracies = accuracy.data$RTnames(accuracies) = accuracy.data$TrialIdaccuracy.key$Accuracy = accuracies[accuracy.key$TrialId]rt.data = merge(rt.data,accuracy.key,by=c("TrialId"))################################################################################  ######## Analysis of RT data:######## ################################################################################  ######## What is RT distribution?truehist(rt.data$RT)######## Identify Ss with reading strategies that indicate lack of comprehension.subjrt.sds = tapply(rt.data$RT,list(rt.data$Subject),sd)bad.subjects = c(bad.subjects,names(subjrt.sds[which(subjrt.sds < 50)]))######## Remove all participants identified in previous stages of data cleaning. 119 Participants leftrt.data = droplevels(subset(rt.data,SID %in% good.subjects & !(Subject %in% bad.subjects)))######## Fit residualization model to filler data######## Data exploration suggests log of region, rather than straight region, would be better fit. There is large discrepency between average RTs on first region and those on subsequent regions######## Also leaving in interaction term of length and region. Model suggests that effect of length is significantly modulated by position in sentence, with less of an impact later on.######## TO DO: Implement LMER. Prediction isn't working yet, needs to be updated. resid.Model = lmer(RT~cLength*cLogRegion+(1|Subject),data=subset(rt.data, Experiment == 'f'))######## With residual model fit, now get rid of all filler / practice trialsrt.data = droplevels(subset(rt.data, Experiment %in% c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8")))rt.data$residRT = rt.data$RT - predict(resid.Model,rt.data)######## Put in contrasts; simple difference coding.rt.data = droplevels(merge(rt.data,condition.key,by.x = "Experiment", by.y = "Experiment", sort=F))######## Outlier rejection. Trim RTs above 5000 and below 100, and apply z-score of 3 by region.######## Also remove 'bad trials', trials where a single region was read for less than 100 msrt.data$Accuracy[which(is.na(rt.data$Accuracy))] = 1bad.trials = unique(rt.data$TrialId[which(rt.data$RT < 100)])rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100))regions = data.frame(Region=c(1:7),ROI=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data = merge(rt.data,regions,by="Region")rt.data = zscoreRegion(rt.data,cutoff=3)rt.data$ROI = factor(rt.data$ROI,levels=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data$logRT = log(rt.data$RT)######## Get by participant means, SEs for raw RTs. SEs are computed using Bakeman + McArthur correction.data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=RT)data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
myANOVA = ezANOVA(data=subset(rt.data,ROI=='SPILL'&Gram=="U"),dv=.(RT),wid=.(Subject),within=.(Dep,Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='SPILL'&Gram=="U"),dv=.(logRT),wid=.(Subject),within=.(Dep,Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='SPILL'&Gram=="U"),dv=.(residRT),wid=.(Subject),within=.(Dep,Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='V+DEP'&Gram=="U"),dv=.(logRT),wid=.(Subject),within=.(Dep,Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='V+DEP'&Gram=="U"),dv=.(RT),wid=.(Subject),within=.(Dep,Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='V+DEP'&Gram=="U"),dv=.(residRT),wid=.(Subject),within=.(Dep,Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='V+DEP'),dv=.(logRT),wid=.(Subject),within=.(Dep,Gram,Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='SPILL'),dv=.(logRT),wid=.(Subject),within=.(Dep,Gram,Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='SPILL'&Dep=="Ref"&Gram=="U"),dv=.(logRT),wid=.(Subject),within=.(Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='V+DEP'&Dep=="Ref"&Gram=="U"),dv=.(logRT),wid=.(Subject),within=.(Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='V+DEP'&Dep=="Agr"&Gram=="U"),dv=.(logRT),wid=.(Subject),within=.(Intr),return_aov=T) myANOVA$ANOVA
by.subj.means = with(subset(rt.data,ROI=="V+DEP"),tapply(logRT,list(Subject,Experiment),mean))by.subj.diffs = by.subj.means[,7]-by.subj.means[,5]cohens.d = mean(by.subj.diffs)/sd(by.subj.diffs)
by.subj.means = with(subset(rt.data,ROI=="V+DEP"),tapply(logRT,list(Subject,Experiment),mean))by.subj.diffs = by.subj.means[,7]-by.subj.means[,6]cohens.d = mean(by.subj.diffs)/sd(by.subj.diffs)
by.subj.diffs
mean(by.subj.diffs)
by.subj.means = with(subset(rt.data,ROI=="V+DEP"),tapply(RT,list(Subject,Experiment),mean))by.subj.diffs = by.subj.means[,7]-by.subj.means[,6]cohens.d = mean(by.subj.diffs)/sd(by.subj.diffs)
mean(by.subj.diffs)
by.subj.means = with(subset(rt.data,ROI=="V+DEP"),tapply(RT,list(Subject,Experiment),mean))by.subj.diffs = by.subj.means[,8]-by.subj.means[,7]cohens.d = mean(by.subj.diffs)/sd(by.subj.diffs)
mean(by.subj.diffs)
by.subj.means = with(subset(rt.data,ROI=="V+DEP"),tapply(RT,list(Subject,Experiment),mean))by.subj.diffs = by.subj.means[,4]-by.subj.means[,3]cohens.d = mean(by.subj.diffs)/sd(by.subj.diffs)
mean(by.subj.diffs)
model.vdep = lmer(logRT~Dep*Intr+(1+ Dep*Intr |Subject)+(1+ Dep*Intr |Item),data = subset(rt.data,ROI=="OBJ"&Gram=="U"))
model.vdep = lmer(logRT~Dep*Intr+(1 |Subject)+(1 |Item),data = subset(rt.data,ROI=="OBJ"&Gram=="U"))
summary(model.vdep)
library(rstan)library(glmer2stan)rt.data$Subject.index = as.integer(as.factor(rt.data$Subject))rt.data$Item.index = as.integer(as.factor(rt.data$Item))stan.fit <- glmer2stan(logRT ~ (1|Item.index) + (1|Subject.index) + Gram,                        data=subset(rt.data,ROI=="V+DEP"))stanmer(stan.fit)
plot(stan.fit)
myModel = lmer(logRT ~ (1 | Item.index) + (1 | Subject.index) + Gram,data=subset(rt.data,ROI=='V+DEP'))
summary(myModel)
plot(stan.fit)
power.t.test(delta=.23,power=.8)
power.t.test(delta=.23,power=.6)
power.t.test(delta=.23,power=.7)
power.t.test(delta=.23,power=.8,alternative='two.sided')
power.t.test(delta=.23,power=.8,alternative='one.sided')
speakers = c("Malayalam","Telugu","Tamil","Malayalam","Malayalam","Gujarati","Hindi","Malayalam","Telugu","Telugu","Tamil","Tamil","Telugu","Telugu","Malayalam","Magahi","Tamil","Telugu","Hindi","Malayalam","Tamil","Urdu","Telugu","Gujarati","Tamil","Malayalam","Tamil","Tamil","Malayalam","Malayalam","English","Malayalam","Malayalam","Tamil","Malayalam","Tamil","Malayalam","Malayalam","Gujarati","Malayalam","Tamil","Tamil","Malayalam","Tamil","Kannada","Tamil","Tamil","Urdu","Tamil","Tamil","Gujarati","Kannada","Tamil","Malayalam","Tamil")
tab = tabulate(speakers)
?tabulate
tab = table(speakers)
tab
ggplot(data=tab) + geom_bar(stat="identity")
ggplot(aes(x=names(tab), y=tab)) + geom_bar(stat="identity")
ggplot(tab,aes(x=names(tab), y=tab)) + geom_bar(stat="identity")
df = c(x = names(tab),y=tab)
df
df = c(x = names(tab),y=as.integer(tab))
df
tab
t  = as.data.frame(tab)
t
ggplot(t,aes(x=speakers, y=Freq)) + geom_bar(stat="identity")
ggplot(t,aes(x=speakers, y=Freq)) + geom_bar(stat="identity")+title("Home language of Indians on MTurk (n = 55)")
ggplot(t,aes(x=speakers, y=Freq)) + geom_bar(stat="identity")+ggtitle("Home language of Indians on MTurk (n = 55)")
dom = c("Malayalam","Telugu","English","Malayalam","English","English","Hindi","English","Hindi","Tamil","Tamil","Tamil","Telugu","Telugu","English","Hindi","Tamil","Tamil","English","English","Tamil","English","English","Tamil","Tamil","Tamil","Tamil","Malayalam","Malayalam","Hindi","Malayalam","English","English","Malayalam","Tamil","Malayalam","English","English","English","Tamil","Tamil","Malayalam","Tamil","English","Tamil","Tamil","Telugu","Tamil","Tamil","English","English","Tamil","English","English")
tab = table(dom)
d = as.data.frame(d)
d = as.data.frame(tab)
par(mfrow=c(2,1))
ggplot(t,aes(x=speakers, y=Freq)) + geom_bar(stat="identity")+ggtitle("Home language of Indians on MTurk (n = 55)")
par(mfrow=c(2,1))
ggplot(t,aes(x=speakers, y=Freq)) + geom_bar(stat="identity")+ggtitle("Home language of Indians on MTurk (n = 55)")
?par
par(mfrow=c(1,2))
ggplot(t,aes(x=speakers, y=Freq)) + geom_bar(stat="identity")+ggtitle("Home language of Indians on MTurk (n = 55)")
par(mfrow=c(1,2))
ggplot(t,aes(x=speakers, y=Freq)) + geom_bar(stat="identity")+ggtitle("Home language of Indians on MTurk (n = 55)")require(gridExtra)#
plot1 <- qplot(1)#
plot2 <- qplot(1)#
grid.arrange(plot1, plot2, ncol=2)
require(gridExtra)#
plot1 <- qplot(1)#
plot2 <- qplot(1)#
grid.arrange(plot1, plot2, ncol=2)
plot1 = ggplot(t,aes(x=speakers, y=Freq)) + geom_bar(stat="identity")+ggtitle("Home language of Indians on MTurk (n = 55)")require(gridExtra)
plot1 = ggplot(t,aes(x=speakers, y=Freq)) + geom_bar(stat="identity")+ggtitle("Home language of Indians on MTurk (n = 55)")
plot2 = ggplot(d,aes(x=speakers, y=Freq)) + geom_bar(stat="identity")+ggtitle("Dominant language of Indians on MTurk (n = 55)")
grid.arrange(plot1, plot2, nrow=2)
d
ggplot(d,aes(x=speakers, y=Freq)) + geom_bar(stat="identity")+ggtitle("Dominant language of Indians on MTurk (n = 55)")
plot2 = ggplot(d,aes(x=dom, y=Freq)) + geom_bar(stat="identity")+ggtitle("Dominant language of Indians on MTurk (n = 55)")
grid.arrange(plot1, plot2, nrow=2)
Set up and load data.######## Need to have 'SPRfnx.R' in local directory.######## 		######## 		################################################################################  ######## Set working directory, and load the (many) packages for the analysis.rm(list=ls())setwd("~/Dropbox/CSL/Agreement and Reflexives/Online_Replication/")library(ez)library(ggplot2)library(plyr)library(ggthemes)library(lme4)library(MASS)library(ggmap)source("SPRfnx.R")######## Read in and parse results file; column names may need to be adjusted if this script is being adapted to new experimentsmycols = c("Subject","MD5","TrialType","Number","Element","Experiment","Item","Region","Response","RT","Judgment")results=read.csv('results_6Ss.txt',header = 0, sep = ",", comment.char = "#",col.names=mycols);summary(results)################################################################################  ######## This part parses the biographical data. It attempts to identify:######## 		i) non-native speakers######## 		ii) participants who have taken the test more than once######## 		iii) participants who got the instruction questions incorrect ######## ######## 		it makes a list 'good.subjects' that contains participants who will be included in the analysis.######## ################################################################################  ######## First get biographical databiography = droplevels(subset(results,Experiment=="intro"|Experiment=="exit"))biography = biography[,c("Subject","Region","Response")]colnames(biography) = c("Subject","Question","Response")biography = reshape(biography,direction="wide",idvar="Subject",v.names="Response",timevar="Question")summary(biography)######## Rename columns and pull out subset of relevant fieldscolnames(biography) = c("Subject","Age","NativeLang","State","ParentLang","DominantLang","OtherLang","Consent","Gender","Question1","Question2","Question3","Question4","Question5","Question6","RandomField","Worker_ID","Identifier")short.biography = biography[,c("Subject","NativeLang","ParentLang","Age","Gender","State","Question1","Question2","Question3","Question4","Question5","Question6","Worker_ID")]short.biography$Age = as.integer(as.character(short.biography$Age))short.biography$SID = c(1:nrow(short.biography))######## Check that the length of unique worker IDs matches length of # of subjects######## 6 participants removed on this criterion.nrow(short.biography) == length(unique(short.biography$Worker_ID))good.subjects = short.biography$SID[!(duplicated(short.biography$Worker_ID))]######## Check for accuracy on instruction questions. Remove participants who answered any incorrectly.######## 11 participants removed on this criterioncorrect.answers = c("space","silent","no","j","yes","maximized")names(correct.answers) = c("Question1","Question2","Question3","Question4","Question5","Question6")all.answers = as.matrix(short.biography[,c("Question1","Question2","Question3","Question4","Question5","Question6")])accurate.answers = apply(all.answers, 1, identical, correct.answers) good.subjects = intersect(good.subjects,which(accurate.answers))######## Leaves us with an N of 126.######## Age analysis: there is a mode of participants 50+. Range = 19-68.dev.new(width =10.761468,height = 4.477064)hg_dot(short.biography$Age[short.biography$SID %in% good.subjects],breaks = seq(17.5,77.5,by=1),cex=2)######## Location analysis######## Someone put in 'ON'. Assumed it was typo for 'OH'states = tolower(short.biography$State)states =  gsub(" $","", states, perl=T)for (i in 1:length(states)) {	if (nchar(states[i]) > 2) {		states[i] = state.abb[match(states[i] ,tolower(state.name))]	}}states = toupper(states)states = gsub("ON","OH",states)state.numbers = table(states[good.subjects])state.data = data.frame(states = state.numbers)state.data$x = state.center$x[match(state.data$states.Var1 ,state.abb)]state.data$y = state.center$y[match(state.data$states.Var1 ,state.abb)]names(state.data) = c("State","Frequency","x","y")dev.new(width =10.752293,height = 10.752293)map = get_map(location = 'USA', zoom = 4,maptype="roadmap")ggmap(map) + geom_point(aes(x = x, y = y, size = Frequency), data = state.data, alpha = .5)+ scale_size(range = c(0, 30))################################################################################  ######## This part turns to analysis of experimental data:######## ################################################################################  ######## Test latin squaring; check... only a tad off.items = droplevels(subset(results,TrialType == "QuestionAlt"))xtabs(~Item+Experiment,data=items)######## Merge biography information into data; create factors, trialIDs. Remove all bad subjects identified above.results = droplevels(subset(results,SID in good.subjects))results = merge(results,short.biography,by.x="Subject",by.y="Subject")results$Length = nchar(as.character(results$Response))results$Subject = as.factor(results$Subject)results$Item = as.factor(results$Item)results$RT = as.numeric(as.character(results$RT))results$TrialId = paste(results$SID,results$Item,sep=":")results$cLength = results$Length - mean(results$Length)######## Separate data into accuracy data and rt.dataaccuracy.data = droplevels(subset(results,TrialType == "QuestionAlt"))rt.data = droplevels(subset(results, TrialType == "DashedSentence"))rt.data$NumRegion = as.numeric(as.character(rt.data$Region))rt.data$logRegion = log(rt.data$NumRegion)rt.data$cLogRegion = rt.data$logRegion - mean(rt.data$logRegion)################################################################################  ######## Analysis of accuracy data:######## ################################################################################  condition.key = data.frame(	Experiment = c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8")	,Dep=c("Agr","Agr","Agr","Agr","Ref","Ref","Ref","Ref"),	Gram=c("G","G","U","U","G","G","U","U"),	Intr=c("S","P","S","P","S","P","S","P"))accuracy.data = droplevels(merge(accuracy.data,condition.key,by.x = "Experiment", by.y = "Experiment", sort=F))######## Remove questions, test accuracycontrasts(accuracy.data$Dep) = contr.sum(levels(accuracy.data$Dep))/2contrasts(accuracy.data$Gram) = contr.sum(levels(accuracy.data$Gram))/2contrasts(accuracy.data$Intr) = contr.sum(levels(accuracy.data$Intr))/2######## Mean response accuracy, tabulated by conditiontapply(accuracy.data$RT,list(accuracy.data$Gram,accuracy.data$Intr,accuracy.data$Dep),mean)######## By participant accuracy, to remove under-performing Sssubj.accuracies = sort(tapply(accuracy.data$RT,list(accuracy.data$Subject),mean))bad.subjects = names(subj.accuracies[which(subj.accuracies < .70)])######## Analysis of accuracy data.#acc.model = glmer(RT~Dep*Gram*Intr+(1|Subject)+(1|Item),data=accuracy.data,family="binomial")######## Tag trials with accuracy######## TO DO: Check that this is working propery. accuracy.key = data.frame(TrialId = unique(results$TrialId),Accuracy = 'NA')accuracies = accuracy.data$RTnames(accuracies) = accuracy.data$TrialIdaccuracy.key$Accuracy = accuracies[accuracy.key$TrialId]rt.data = merge(rt.data,accuracy.key,by=c("TrialId"))################################################################################  ######## Analysis of RT data:######## ################################################################################  ######## What is RT distribution?truehist(rt.data$RT)######## Identify Ss with reading strategies that indicate lack of comprehension.subjrt.sds = tapply(rt.data$RT,list(rt.data$Subject),sd)bad.subjects = c(bad.subjects,names(subjrt.sds[which(subjrt.sds < 50)]))######## Remove all participants identified in previous stages of data cleaning. 119 Participants leftrt.data = droplevels(subset(rt.data,SID %in% good.subjects & !(Subject %in% bad.subjects)))######## Fit residualization model to filler data######## Data exploration suggests log of region, rather than straight region, would be better fit. There is large discrepency between average RTs on first region and those on subsequent regions######## Also leaving in interaction term of length and region. Model suggests that effect of length is significantly modulated by position in sentence, with less of an impact later on.######## TO DO: Implement LMER. Prediction isn't working yet, needs to be updated. resid.Model = lmer(RT~cLength*cLogRegion+(1|Subject),data=subset(rt.data, Experiment == 'f'))######## With residual model fit, now get rid of all filler / practice trialsrt.data = droplevels(subset(rt.data, Experiment %in% c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8")))rt.data$residRT = rt.data$RT - predict(resid.Model,rt.data)######## Put in contrasts; simple difference coding.rt.data = droplevels(merge(rt.data,condition.key,by.x = "Experiment", by.y = "Experiment", sort=F))######## Outlier rejection. Trim RTs above 5000 and below 100, and apply z-score of 3 by region.######## Also remove 'bad trials', trials where a single region was read for less than 100 msrt.data$Accuracy[which(is.na(rt.data$Accuracy))] = 1bad.trials = unique(rt.data$TrialId[which(rt.data$RT < 100)])rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100))regions = data.frame(Region=c(1:7),ROI=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data = merge(rt.data,regions,by="Region")rt.data = zscoreRegion(rt.data,cutoff=3)rt.data$ROI = factor(rt.data$ROI,levels=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data$logRT = log(rt.data$RT)######## Get by participant means, SEs for raw RTs. SEs are computed using Bakeman + McArthur correction.data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=RT)data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")######## What is Cohen's d effect size of agreement intrusion effect?
by.subj.means = with(subset(rt.data,ROI=="V+DEP"),tapply(logRT,list(Subject,Experiment),mean))by.subj.diffs = by.subj.means[,7]-by.subj.means[,5]cohens.d = mean(by.subj.diffs)/sd(by.subj.diffs)
rt.data = merge(rt.data,item.verbs,by="Item")
item.verbs = data.frame(Item = c(1:48),Verb = c("cared","sang","advised","worked","helped","attended","studied","interviewed","assisted","glared","taught","babysat","promoted","danced","stalked","chastised","lectured","advised","treated","promoted","socialized","worked","assisted","worked","served","entertained","yelled","accompanied","scolded","aided","listened","trained","worked","trained","escorted","argued","worked","supervised","advised","oversaw","campaigned","conversed","employed","helped","treated","argued","helped","avoided"))rt.data = merge(rt.data,item.verbs,by="Item")
data.means.byItem = ddply(rt.data,.(Item,Experiment,ROI),summarize,trialRT=RT)grand.mean = mean(data.means.byItem$trialRT)S.means = tapply(data.means.byItem$trialRT,list(data.means.byItem$Item),mean)data.means.byItem$trialRT = (data.means.byItem$trialRT - S.means[data.means.byItem$Item])+grand.meandata.means = ddply(data.means.byItem,.(ROI,Experiment),summarize,meanRT=mean(trialRT),rtSE=sd(trialRT)/sqrt(length(levels(rt.data$Item))))
ggplot(data.means,aes(x=Item,y=trialRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
data.means.byItem = ddply(subset(rt.data,ROI=="V+DEP"),.(Item,Experiment),summarize,trialRT=RT,rtSE=sd(RT)/sqrt(length(levels(rt.data$Item)))dodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=Item,y=trialRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means.byItem,aes(x=Item,y=trialRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
data.means.byItem = ddply(subset(rt.data,ROI=="V+DEP"&Experiment %in% c("mockingbird-7","mockingbird-8")),.(Item,Experiment),summarize,trialRT=RT,rtSE=sd(RT)/sqrt(length(levels(rt.data$Item)))dodge <- position_dodge(width=0.9)ggplot(data.means.byItem,aes(x=Item,y=trialRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
data.means.byItem = ddply(subset(rt.data,ROI=="V+DEP" & Experiment %in% c("mockingbird-7","mockingbird-8")),.(Item,Experiment),summarize,trialRT=RT,rtSE=sd(RT)/sqrt(length(levels(rt.data$Item)))
data.means.byItem = ddply(subset(rt.data,ROI=="V+DEP" & Experiment %in% c("mockingbird-7","mockingbird-8")),.(Item,Experiment),summarize,trialRT=RT,rtSE=sd(RT)/sqrt(length(levels(rt.data$Item))))
data.means.byItem = ddply(subset(rt.data,ROI=="V+DEP" & Experiment %in% c("mockingbird-7","mockingbird-8")),.(Item,Experiment),summarize,trialRT=RT,rtSE=sd(RT)/sqrt(length(levels(rt.data$Item))))
data.means.byItem = ddply(subset(rt.data, ROI=="V+DEP" & (Experiment %in% c("mockingbird-7","mockingbird-8"))),.(Item,Experiment),summarize,trialRT=RT,rtSE=sd(RT)/sqrt(length(levels(rt.data$Item)))
)
(subset(rt.data, ROI=="V+DEP" & Experiment %in% c("mockingbird-7","mockingbird-8") ),.(Item,Experiment),summarize,trialRT=RT)
data.means.byItem = ddply(subset(rt.data, ROI=="V+DEP" & Experiment %in% c("mockingbird-7","mockingbird-8") ).(Item,Experiment),summarize,trialRT=RT)
data.means.byItem = ddply(subset(rt.data, ROI=="V+DEP" & Experiment %in% c("mockingbird-7","mockingbird-8"),.(Item,Experiment),summarize,trialRT=RT)
)
data.means.byItem = ddply(subset(rt.data, ROI=="V+DEP" & Experiment %in% c("mockingbird-7","mockingbird-8")),.(Item,Experiment),summarize,trialRT=RT)
dodge <- position_dodge(width=0.9)ggplot(data.means.byItem,aes(x=Item,y=trialRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
data.means.byItem = ddply(subset(rt.data, ROI=="V+DEP" & Experiment %in% c("mockingbird-3","mockingbird-4")),.(Item,Experiment),summarize,trialRT=RT)dodge <- position_dodge(width=0.9)ggplot(data.means.byItem,aes(x=Item,y=trialRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
data.means.byItem = ddply(subset(rt.data, ROI=="V+DEP" & Experiment %in% c("mockingbird-3","mockingbird-4")),.(Verb,Experiment),summarize,trialRT=RT)dodge <- position_dodge(width=0.9)ggplot(data.means.byItem,aes(x=Verb,y=trialRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
data.means.byItem = ddply(subset(rt.data, ROI=="V+DEP" & Experiment %in% c("mockingbird-7","mockingbird-8")),.(Verb,Experiment),summarize,trialRT=RT)dodge <- position_dodge(width=0.9)ggplot(data.means.byItem,aes(x=Verb,y=trialRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means.byItem,aes(x=Verb,y=trialRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")+opts(axis.text.x=theme_text(angle=-45))
myANOVA = ezANOVA(data=subset(rt.data,ROI=='SPILL'&Gram=="U"),dv=.(RT),wid=.(Item),within=.(Dep,Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='SPILL'&Gram=="U"&Dep=="Ref"),dv=.(logRT),wid=.(Item),within=.(Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='SPILL'&Gram=="U"&Dep=="Agr"),dv=.(logRT),wid=.(Item),within=.(Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='V+DEP'&Gram=="U"&Dep=="Agr"),dv=.(logRT),wid=.(Item),within=.(Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='V+DEP'&Gram=="U"&Dep=="Ref"),dv=.(logRT),wid=.(Item),within=.(Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='V+DEP'&Gram=="U"),dv=.(RT),wid=.(Item),within=.(Dep,Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='V+DEP'&Gram=="U"),dv=.(logRT),wid=.(Item),within=.(Dep,Intr),return_aov=T) myANOVA$ANOVA
myANOVA = ezANOVA(data=subset(rt.data,ROI=='V+DEP'&Gram=="U"),dv=.(residRT),wid=.(Item),within=.(Dep,Intr),return_aov=T) myANOVA$ANOVA
means = tapply(rt.data$RT,list(rt.data$Item,rt.data$ROI,rt.data$Experiment),mean)
?apply
allMeans = tapply(rt.data$RT,list(rt.data$Subject,rt.data$Item,rt.data$ROI,rt.data$Experiment),mean)
allMeans
apply(allMeans[,7]-allMeans[,8],1,mean,na.rm=T)
apply(allMeans[,,7]-allMeans[,,8],1,mean,na.rm=T)
apply(allMeans[,,7]-allMeans[,,8],1,mean,na.rm=T)
apply(allMeans[,,,7]-allMeans[,,8],1,mean,na.rm=T)
apply(allMeans[,,,7]-allMeans[,,,8],1,mean,na.rm=T)
?apply
apply(allMeans[,,,7]-allMeans[,,,8],1,mean,na.rm=TRUE)
apply(allMeans[,,,7]-allMeans[,,,8],2,mean,na.rm=TRUE)
item.diffs = data.frame(Item=c(1:48),Diff = means[,7]-means[,8],SD = sd(means[,7]-means[,8]))
means = tapply(rt.data$RT,list(rt.data$Item,rt.data$ROI,rt.data$Experiment),mean)
item.diffs = data.frame(Item=c(1:48),Diff = means[,"V+DEP",7]-means[,"V+DEP",8],SD = sd(means[,"V+DEP",7]-means[,"V+DEP",8]))
ggplot(item.diffs,aes(x=Item,y=Diff))+geom_linerange(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")+opts(axis.text.x=theme_text(angle=-45))
ggplot(item.diffs,aes(x=Item,y=Diff))+geom_linerange(position="dodge",stat = "identity",ymin=Diff-SD,ymax=Diff+SD) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
item.diffs
ggplot(item.diffs,aes(x=Item,y=Diff))+geom_pointrange(position="dodge",stat = "identity) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
means = tapply(rt.data$RT,list(rt.data$Item,rt.data$ROI,rt.data$Experiment),mean)ggplot(item.diffs,aes(x=Item,y=Diff))+geom_pointrange(position="dodge",stat = "identity) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(item.diffs,aes(x=Item,y=Diff))+geom_pointrange(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(item.diffs,aes(x=Item,y=Diff))+geom_point(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
item.diffs = data.frame(Item=c(1:48),Diff = means[,"V+DEP",3]-means[,"V+DEP",4],SD = sd(means[,"V+DEP",7]-means[,"V+DEP",8])) ggplot(item.diffs,aes(x=Item,y=Diff))+geom_point(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
item.diffs = data.frame(Item=c(1:48),Diff = means[,"V+DEP",7]-means[,"V+DEP",8],SD = sd(means[,"V+DEP",7]-means[,"V+DEP",8])) ggplot(item.diffs,aes(x=Item,y=Diff))+geom_point(position="dodge",stat = "identity") + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
rm(list=ls())setwd("~/Dropbox/CSL/Agreement and Reflexives/Online_Replication/")library(ez)library(ggplot2)library(plyr)library(ggthemes)library(lme4)library(MASS)library(ggmap)source("SPRfnx.R")######## Read in and parse results file; column names may need to be adjusted if this script is being adapted to new experimentsmycols = c("Subject","MD5","TrialType","Number","Element","Experiment","Item","Region","Response","RT","Judgment")results=read.csv('results_6Ss.txt',header = 0, sep = ",", comment.char = "#",col.names=mycols);summary(results)################################################################################  ######## This part parses the biographical data. It attempts to identify:######## 		i) non-native speakers######## 		ii) participants who have taken the test more than once######## 		iii) participants who got the instruction questions incorrect ######## ######## 		it makes a list 'good.subjects' that contains participants who will be included in the analysis.######## ################################################################################  ######## First get biographical databiography = droplevels(subset(results,Experiment=="intro"|Experiment=="exit"))biography = biography[,c("Subject","Region","Response")]colnames(biography) = c("Subject","Question","Response")biography = reshape(biography,direction="wide",idvar="Subject",v.names="Response",timevar="Question")summary(biography)######## Rename columns and pull out subset of relevant fieldscolnames(biography) = c("Subject","Age","NativeLang","State","ParentLang","DominantLang","OtherLang","Consent","Gender","Question1","Question2","Question3","Question4","Question5","Question6","RandomField","Worker_ID","Identifier")short.biography = biography[,c("Subject","NativeLang","ParentLang","Age","Gender","State","Question1","Question2","Question3","Question4","Question5","Question6","Worker_ID")]short.biography$Age = as.integer(as.character(short.biography$Age))short.biography$SID = c(1:nrow(short.biography))######## Check that the length of unique worker IDs matches length of # of subjects######## 6 participants removed on this criterion.nrow(short.biography) == length(unique(short.biography$Worker_ID))good.subjects = short.biography$SID[!(duplicated(short.biography$Worker_ID))]######## Check for accuracy on instruction questions. Remove participants who answered any incorrectly.######## 11 participants removed on this criterioncorrect.answers = c("space","silent","no","j","yes","maximized")names(correct.answers) = c("Question1","Question2","Question3","Question4","Question5","Question6")all.answers = as.matrix(short.biography[,c("Question1","Question2","Question3","Question4","Question5","Question6")])accurate.answers = apply(all.answers, 1, identical, correct.answers) good.subjects = intersect(good.subjects,which(accurate.answers))######## Leaves us with an N of 126.######## Age analysis: there is a mode of participants 50+. Range = 19-68.dev.new(width =10.761468,height = 4.477064)hg_dot(short.biography$Age[short.biography$SID %in% good.subjects],breaks = seq(17.5,77.5,by=1),cex=2)######## Location analysis######## Someone put in 'ON'. Assumed it was typo for 'OH'states = tolower(short.biography$State)states =  gsub(" $","", states, perl=T)for (i in 1:length(states)) {	if (nchar(states[i]) > 2) {		states[i] = state.abb[match(states[i] ,tolower(state.name))]	}}states = toupper(states)states = gsub("ON","OH",states)state.numbers = table(states[good.subjects])state.data = data.frame(states = state.numbers)state.data$x = state.center$x[match(state.data$states.Var1 ,state.abb)]state.data$y = state.center$y[match(state.data$states.Var1 ,state.abb)]names(state.data) = c("State","Frequency","x","y")dev.new(width =10.752293,height = 10.752293)map = get_map(location = 'USA', zoom = 4,maptype="roadmap")ggmap(map) + geom_point(aes(x = x, y = y, size = Frequency), data = state.data, alpha = .5)+ scale_size(range = c(0, 30))################################################################################  ######## This part turns to analysis of experimental data:######## ################################################################################  ######## Test latin squaring; check... only a tad off.items = droplevels(subset(results,TrialType == "QuestionAlt"))xtabs(~Item+Experiment,data=items)######## Merge biography information into data; create factors, trialIDs. Remove all bad subjects identified above.results = droplevels(subset(results,SID in good.subjects))results = merge(results,short.biography,by.x="Subject",by.y="Subject")results$Length = nchar(as.character(results$Response))results$Subject = as.factor(results$Subject)results$Item = as.factor(results$Item)results$RT = as.numeric(as.character(results$RT))results$TrialId = paste(results$SID,results$Item,sep=":")results$cLength = results$Length - mean(results$Length)######## Separate data into accuracy data and rt.dataaccuracy.data = droplevels(subset(results,TrialType == "QuestionAlt"))rt.data = droplevels(subset(results, TrialType == "DashedSentence"))rt.data$NumRegion = as.numeric(as.character(rt.data$Region))rt.data$logRegion = log(rt.data$NumRegion)rt.data$cLogRegion = rt.data$logRegion - mean(rt.data$logRegion)################################################################################  ######## Analysis of accuracy data:######## ################################################################################  condition.key = data.frame(	Experiment = c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8")	,Dep=c("Agr","Agr","Agr","Agr","Ref","Ref","Ref","Ref"),	Gram=c("G","G","U","U","G","G","U","U"),	Intr=c("S","P","S","P","S","P","S","P"))accuracy.data = droplevels(merge(accuracy.data,condition.key,by.x = "Experiment", by.y = "Experiment", sort=F))######## Remove questions, test accuracycontrasts(accuracy.data$Dep) = contr.sum(levels(accuracy.data$Dep))/2contrasts(accuracy.data$Gram) = contr.sum(levels(accuracy.data$Gram))/2contrasts(accuracy.data$Intr) = contr.sum(levels(accuracy.data$Intr))/2######## Mean response accuracy, tabulated by conditiontapply(accuracy.data$RT,list(accuracy.data$Gram,accuracy.data$Intr,accuracy.data$Dep),mean)######## By participant accuracy, to remove under-performing Sssubj.accuracies = sort(tapply(accuracy.data$RT,list(accuracy.data$Subject),mean))bad.subjects = names(subj.accuracies[which(subj.accuracies < .70)])######## Analysis of accuracy data.#acc.model = glmer(RT~Dep*Gram*Intr+(1|Subject)+(1|Item),data=accuracy.data,family="binomial")######## Tag trials with accuracy######## TO DO: Check that this is working propery. accuracy.key = data.frame(TrialId = unique(results$TrialId),Accuracy = 'NA')accuracies = accuracy.data$RTnames(accuracies) = accuracy.data$TrialIdaccuracy.key$Accuracy = accuracies[accuracy.key$TrialId]rt.data = merge(rt.data,accuracy.key,by=c("TrialId"))################################################################################  ######## Analysis of RT data:######## ################################################################################  ######## What is RT distribution?truehist(rt.data$RT)######## Identify Ss with reading strategies that indicate lack of comprehension.subjrt.sds = tapply(rt.data$RT,list(rt.data$Subject),sd)bad.subjects = c(bad.subjects,names(subjrt.sds[which(subjrt.sds < 50)]))######## Remove all participants identified in previous stages of data cleaning. 119 Participants leftrt.data = droplevels(subset(rt.data,SID %in% good.subjects & !(Subject %in% bad.subjects)))######## Fit residualization model to filler data######## Data exploration suggests log of region, rather than straight region, would be better fit. There is large discrepency between average RTs on first region and those on subsequent regions######## Also leaving in interaction term of length and region. Model suggests that effect of length is significantly modulated by position in sentence, with less of an impact later on.######## TO DO: Implement LMER. Prediction isn't working yet, needs to be updated. resid.Model = lmer(RT~cLength*cLogRegion+(1|Subject),data=subset(rt.data, Experiment == 'f'))######## With residual model fit, now get rid of all filler / practice trialsrt.data = droplevels(subset(rt.data, Experiment %in% c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8")))rt.data$residRT = rt.data$RT - predict(resid.Model,rt.data)######## Put in contrasts; simple difference coding.rt.data = droplevels(merge(rt.data,condition.key,by.x = "Experiment", by.y = "Experiment", sort=F))######## Outlier rejection. Trim RTs above 5000 and below 100, and apply z-score of 3 by region.######## Also remove 'bad trials', trials where a single region was read for less than 100 msrt.data$Accuracy[which(is.na(rt.data$Accuracy))] = 1bad.trials = unique(rt.data$TrialId[which(rt.data$RT < 100)])rt.data = droplevels(subset(rt.data, RT < 5000 & RT > 100))regions = data.frame(Region=c(1:7),ROI=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data = merge(rt.data,regions,by="Region")rt.data = zscoreRegion(rt.data,cutoff=3)rt.data$ROI = factor(rt.data$ROI,levels=c("SUBJ","RC VERB","OBJ","ADV","V+DEP","SPILL","SPILL2"))rt.data$logRT = log(rt.data$RT)######## Get by participant means, SEs for raw RTs. SEs are computed using Bakeman + McArthur correction.data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=RT)data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")######## What is Cohen's d effect size of agreement intrusion effect?by.subj.means = with(subset(rt.data,ROI=="V+DEP"),tapply(logRT,list(Subject,Experiment),mean))by.subj.diffs = by.subj.means[,7]-by.subj.means[,5]cohens.d = mean(by.subj.diffs)/sd(by.subj.diffs)######## What is effect by item, and how does it relate to verb inside RC?item.verbs = data.frame(Item = c(1:48),Verb = c("cared","sang","advised","worked","helped","attended","studied","interviewed","assisted","glared","taught","babysat","promoted","danced","stalked","chastised","lectured","advised","treated","promoted","socialized","worked","assisted","worked","served","entertained","yelled","accompanied","scolded","aided","listened","trained","worked","trained","escorted","argued","worked","supervised","advised","oversaw","campaigned","conversed","employed","helped","treated","argued","helped","avoided"))rt.data = merge(rt.data,item.verbs,by="Item")
data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=mean(RT))data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT datadodge <- position_dodge(width=0.9)ggplot(data.means,aes(x=ROI,y=meanRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
data.means.bytrial = ddply(rt.data,.(Subject,Item,Experiment,ROI),summarize,trialRT=mean(RT),trialLogRT=mean(logRT),trialResidRT=mean(residRT))
data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT),subjmeanLogRT=mean(trialLogRT),subjmeanResidRT=mean(trialResidRT))
data.means.by.trial= {}
data.means.by.trial= rt.data
data.means.bysubj = ddply(data.means.bytrial,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(trialRT),subjmeanLogRT=mean(trialLogRT),subjmeanResidRT=mean(trialResidRT))grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))))
grand.means = c(mean(data.means.bysubj$subjmeanRT),mean(data.means.bysubj$subjmeanLogRT),mean(data.means.bysubj$subjmeanResidRT)
_
grand.means = c(mean(data.means.bysubj$subjmeanRT),mean(data.means.bysubj$subjmeanLogRT),mean(data.means.bysubj$subjmeanResidRT))
grand.means
data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.mean
data.means.bysubj = ddply(rt.data,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(RT),subjmeanLogRT=mean(logRT),subjmeanResidRT=mean(residRT))grand.means = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))),meanLogRT=mean(subjmeanLogRT),LogrtSE=sd(subjmeanLogRT)/sqrt(length(levels(rt.data$Subject))),meanResidRT=mean(subjmeanResidRT),ResidrtSE=sd(subjmeanResidRT)/sqrt(length(levels(rt.data$Subject))))
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ylim(6,7)+ geom_errorbar(position=dodge,aes(ymax = meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
data.means.bysubj = ddply(rt.data,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(RT),subjmeanLogRT=mean(logRT),subjmeanResidRT=mean(residRT))######## Bakeman + McArthur correction for all three reading timesgrand.means = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.mean#
grand.means = mean(data.means.bysubj$subjmeanLogRT)S.means = tapply(data.means.bysubj$subjmeanLogRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanLogRT = (data.means.bysubj$subjmeanLogRT - S.means[data.means.bysubj$Subject])+grand.meangrand.means = mean(data.means.bysubj$subjmeanResidRT)S.means = tapply(data.means.bysubj$subjmeanResidRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanResidRT = (data.means.bysubj$subjmeanResidRT - S.means[data.means.bysubj$Subject])+grand.mean
data.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))),meanLogRT=mean(subjmeanLogRT),LogrtSE=sd(subjmeanLogRT)/sqrt(length(levels(rt.data$Subject))),meanResidRT=mean(subjmeanResidRT),ResidrtSE=sd(subjmeanResidRT)/sqrt(length(levels(rt.data$Subject))))
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ylim(6,7)+geom_errorbar(position=dodge,aes(ymax = meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
data.means
data.means$meanLogRT
data.means.bysubj = ddply(rt.data,.(Subject,ROI,Experiment),summarize,subjmeanRT=mean(RT),subjmeanLogRT=mean(logRT),subjmeanResidRT=mean(residRT))
grand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meangrand.mean = mean(data.means.bysubj$subjmeanLogRT)S.means = tapply(data.means.bysubj$subjmeanLogRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanLogRT = (data.means.bysubj$subjmeanLogRT - S.means[data.means.bysubj$Subject])+grand.mean
grand.mean = mean(data.means.bysubj$subjmeanResidRT)S.means = tapply(data.means.bysubj$subjmeanResidRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanResidRT = (data.means.bysubj$subjmeanResidRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Experiment),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))),meanLogRT=mean(subjmeanLogRT),LogrtSE=sd(subjmeanLogRT)/sqrt(length(levels(rt.data$Subject))),meanResidRT=mean(subjmeanResidRT),ResidrtSE=sd(subjmeanResidRT)/sqrt(length(levels(rt.data$Subject))))######## Plot all RT data
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+ylim(6,7)+geom_errorbar(position=dodge,aes(ymax = meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment))+geom_bar(position="dodge",stat = "identity")+geom_errorbar(position=dodge,aes(ymax = meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment))+geom_pointrange(position="dodge",stat = "identity",ymax = meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15)+ylim(6,7)+ theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
head(data.means)
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment))+geom_pointrange(position="dodge",stat = "identity",ymax =meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15)+ylim(6,7)+ theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment))+geom_pointrange(position="dodge",ymax =meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15)+ylim(6,7)+ theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment))+geom_pointrange(position="dodge",data=data.means,stat = "identity",ymax =meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15)+ylim(6,7)+ theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment,ymax =meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15))+geom_pointrange(position="dodge",stat,stat = "identity")+ylim(6,7)+ theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment,ymax =meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15))+geom_pointrange(position="dodge",stat = "identity")+ylim(6,7)+ theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment,ymax =meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15))+geom_pointrange(position=dodge,stat = "identity")+ylim(6,7)+ theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Experiment,ymax =meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15))+geom_pointrange(position=dodge,stat = "identity")+ylim(6,6.5)+ theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
label.key = data.frame(Experiment = c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8"),Label=c("Agr:G,S","Agr:G,PL","Agr:U,S","Agr:U,P","Ref:G,S","Ref:G,PL","Ref:U,S","Ref:U,P")
label.key = data.frame(Experiment = c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8"),Label=c("Agr:G,S","Agr:G,PL","Agr:U,S","Agr:U,P","Ref:G,S","Ref:G,PL","Ref:U,S","Ref:U,P")rt.data = merge(rt.data,label.key,by = "Experiment")
label.key = data.frame(Experiment = c("mockingbird-1","mockingbird-2","mockingbird-3","mockingbird-4","mockingbird-5","mockingbird-6","mockingbird-7","mockingbird-8"),Label=c("Agr:G,S","Agr:G,PL","Agr:U,S","Agr:U,P","Ref:G,S","Ref:G,PL","Ref:U,S","Ref:U,P"))rt.data = merge(rt.data,label.key,by = "Experiment")
data.means.bysubj = ddply(rt.data,.(Subject,ROI,Label),summarize,subjmeanRT=mean(RT),subjmeanLogRT=mean(logRT),subjmeanResidRT=mean(residRT))######## Bakeman + McArthur correction for all three reading timesgrand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meangrand.mean = mean(data.means.bysubj$subjmeanLogRT)S.means = tapply(data.means.bysubj$subjmeanLogRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanLogRT = (data.means.bysubj$subjmeanLogRT - S.means[data.means.bysubj$Subject])+grand.meangrand.mean = mean(data.means.bysubj$subjmeanResidRT)S.means = tapply(data.means.bysubj$subjmeanResidRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanResidRT = (data.means.bysubj$subjmeanResidRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Label),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))),meanLogRT=mean(subjmeanLogRT),LogrtSE=sd(subjmeanLogRT)/sqrt(length(levels(rt.data$Subject))),meanResidRT=mean(subjmeanResidRT),ResidrtSE=sd(subjmeanResidRT)/sqrt(length(levels(rt.data$Subject))))
ggplot(data.means,aes(x=ROI,y=meanLogRT,fill=Label,ymax =meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15))+geom_pointrange(position=dodge,stat = "identity")+ylim(6,6.5)+ theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means,aes(x=ROI,y=meanLogRT,color=Label,ymax =meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15))+geom_pointrange(position=dodge,stat = "identity")+ylim(6,6.5)+ theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
rt.data$Label = as.factor(rt.data$Label,levels = c("Agr:G,S","Agr:G,PL","Agr:U,S","Agr:U,P","Ref:G,S","Ref:G,PL","Ref:U,S","Ref:U,P"))
rt.data$Label = factor(rt.data$Label,levels = c("Agr:G,S","Agr:G,PL","Agr:U,S","Agr:U,P","Ref:G,S","Ref:G,PL","Ref:U,S","Ref:U,P"))
data.means.bysubj = ddply(rt.data,.(Subject,ROI,Label),summarize,subjmeanRT=mean(RT),subjmeanLogRT=mean(logRT),subjmeanResidRT=mean(residRT))######## Bakeman + McArthur correction for all three reading timesgrand.mean = mean(data.means.bysubj$subjmeanRT)S.means = tapply(data.means.bysubj$subjmeanRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanRT = (data.means.bysubj$subjmeanRT - S.means[data.means.bysubj$Subject])+grand.meangrand.mean = mean(data.means.bysubj$subjmeanLogRT)S.means = tapply(data.means.bysubj$subjmeanLogRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanLogRT = (data.means.bysubj$subjmeanLogRT - S.means[data.means.bysubj$Subject])+grand.meangrand.mean = mean(data.means.bysubj$subjmeanResidRT)S.means = tapply(data.means.bysubj$subjmeanResidRT,list(data.means.bysubj$Subject),mean)data.means.bysubj$subjmeanResidRT = (data.means.bysubj$subjmeanResidRT - S.means[data.means.bysubj$Subject])+grand.meandata.means = ddply(data.means.bysubj,.(ROI,Label),summarize,meanRT=mean(subjmeanRT),rtSE=sd(subjmeanRT)/sqrt(length(levels(rt.data$Subject))),meanLogRT=mean(subjmeanLogRT),LogrtSE=sd(subjmeanLogRT)/sqrt(length(levels(rt.data$Subject))),meanResidRT=mean(subjmeanResidRT),ResidrtSE=sd(subjmeanResidRT)/sqrt(length(levels(rt.data$Subject))))
ggplot(data.means,aes(x=ROI,y=meanLogRT,color=Label,ymax =meanLogRT+LogrtSE,ymin=meanLogRT-LogrtSE,width=0.15))+geom_pointrange(position=dodge,stat = "identity")+ylim(6,6.5)+ theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
length(levels(rt.data$Subject))
ggplot(data.means,aes(x=ROI,y=meanResidRT,fill=Label))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanRT+rtSE,ymin=meanRT-rtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
ggplot(data.means,aes(x=ROI,y=meanResidRT,fill=Label))+geom_bar(position="dodge",stat = "identity")+ geom_errorbar(position=dodge,aes(ymax = meanResidRT+ResidrtSE,ymin=meanResidRT-ResidrtSE,width=0.15)) + theme_economist() + scale_colour_economist() + ggtitle("Reading time for MB Online")
Sweave(file="~/Dropbox/Current\ Work/RussianExtrapRCs/writeup/russian-extraposed-rcs.Rnw")
setwd('~/Dropbox/Current\ Work/RussianExtrapRCs/writeup/')
Sweave(file="~/Dropbox/Current\ Work/RussianExtrapRCs/writeup/russian-extraposed-rcs.Rnw")
Sweave(file="~/Dropbox/Current\ Work/RussianExtrapRCs/writeup/russian-extraposed-rcs.Rnw")
Sweave(file="~/Dropbox/Current\ Work/RussianExtrapRCs/writeup/russian-extraposed-rcs.Rnw")
